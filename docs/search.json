[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDAV Project",
    "section": "",
    "text": "1 Introduction\nThis project aims to analyze the performance of stocks in each sector for major corporations by identifying trends and understanding the factors influencing stock price variations across various sectors. Stock prices are determined not only by a company’s performance but also by broader market trends, sector-specific dynamics, and external economic factors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "EDAV Project",
    "section": "1.1 Background",
    "text": "1.1 Background\nFor example, greater stock prices are frequently connected with technology companies because investors anticipate innovation and development from these companies. On the other hand, utility firms tend to have lower stock values due to the nature of their business, primarily dependent on consistent demand and regulated pricing. Understanding these distinctions is critical for investors looking to make educated decisions based on sectoral performance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#objectives",
    "href": "index.html#objectives",
    "title": "EDAV Project",
    "section": "1.2 Objectives",
    "text": "1.2 Objectives\nThrough this analysis, we aim to explore:\n\nWhy some firms in the same industry outperform others.\nThe impact of factors such as market sentiment, growth prospects, competitive advantages, and financial soundness on stock performance.\nThe influence of external events such as economic downturns, market booms, and geopolitical crises on stock performance across sectors.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "EDAV Project",
    "section": "1.3 Methodology",
    "text": "1.3 Methodology\nTo achieve these goals, we will:\n\nUtilize bar charts, scatter plots, and sector performance comparisons to visually highlight key insights.\nAnalyze the relative success of each sector over time.\nInvestigate the underlying causes of performance disparities using data-driven approaches.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#expected-outcomes",
    "href": "index.html#expected-outcomes",
    "title": "EDAV Project",
    "section": "1.4 Expected Outcomes",
    "text": "1.4 Expected Outcomes\nBy the end of the research, we aim to present:\n\nA clear, data-driven narrative about how different sectors perform in the market.\nInsights into the variables influencing their success.\n\nThese visual tools and analyses will enable us to better understand sectoral performance and provide actionable insights for informed investment decisions.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "2  Data",
    "section": "",
    "text": "2.1 Description\nFor this project, we will use historical stock data for companies in three key sectors: Technology, Finance, and Healthcare. The dataset will span five years, allowing for a robust analysis of trends over time.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#description",
    "href": "data.html#description",
    "title": "2  Data",
    "section": "",
    "text": "2.1.1 Sectors and Companies\n\n2.1.1.1 Technology:\n\nApple Inc. (AAPL)\nAlphabet Inc. Class C (GOOG)\nMeta Platforms Inc. Class A (META)\nMicrosoft Corp. (MSFT)\nNVIDIA Corporation (NVDA)\n\n\n\n2.1.1.2 Finance:\n\nBank of America (BAC)\nGoldman Sachs Group Inc. (GS)\nJPMorgan Chase & Co. (JPM)\nMorgan Stanley (MS)\nWells Fargo & Co. (WFC)\n\n\n\n2.1.1.3 Healthcare:\n\nAstraZeneca (AZN)\nJohnson & Johnson (JNJ)\nEli Lilly and Co. (LLY)\nMerck & Co., Inc. (MRK)\nPfizer Inc. (PFE)\n\n\n\n\n2.1.2 Data Source\nNasdaq collects data through its exchange from transactions conducted by various market participants, including institutional investors, brokers, market makers, and retail traders. The data encompasses details on market activity such as trades, order books, bid/ask prices, volumes, and timestamped events.\n\n2.1.2.1 Transaction Data\nEach trade, quote, and order is logged, providing insights into the dynamics of stock movements, liquidity, and market trends.\n\n\n2.1.2.2 Participants\nMarket participants, including broker-dealers and investors, generate real-time and historical data through their buying and selling activities.\n\n\n\n2.1.3 Format and Frequency\nThe data will be accessed in formats like CSV, JSON, or XML, which are compatible with analysis tools such as Python or R. The frequency of updates varies:\n\nDaily Data: Updated at the end of each trading day.\nIntraday (Minute-Level) Data: Real-time updates available during market hours.\n\n\n\n2.1.4 Key Data Dimensions\n\nDate: Trading date.\n\nOpen: Opening stock price.\n\nHigh: Highest price during the trading day.\n\nLow: Lowest price during the trading day.\n\nClose/Last: Closing price or last trade price.\n\nVolume: Total shares traded.\n\n\n\n2.1.5 Potential Issues\n\nData Gaps: Missing data during market holidays or after-hours trading.\nAdjustments: Stock splits and dividends may require corrections for consistency.\n\nData Volume: Handling minute-level data over five years for multiple companies may require significant processing power and storage.\n\n\n\n2.1.6 Data Import Plan\nThe data import strategy entails retrieving structured information from an API endpoint using HTTP requests that include appropriate headers to ensure successful communication. The JSON answer is processed and turned into a structured DataFrame, allowing for rapid data manipulation and analysis. Relevant data is then extracted and saved locally in a CSV file for further processing and analysis. This methodical methodology guarantees the smooth integration of raw data from an external source into a useful tabular format.\n\n\n2.1.7 Sources\nNasdaq’s market data services will be the primary source, with potential supplementary data from financial APIs or publicly available CSV datasets.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.2 Missing value analysis",
    "text": "2.2 Missing value analysis\n\n\nCode\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(readr)\n\n\nData fetching:\n\n\nCode\nfetch_and_save_data &lt;- function(api_url, csv_path) {\n  response &lt;- GET(api_url, add_headers(\"User-Agent\" = \"Mozilla/5.0\"))\n  \n  if (status_code(response) == 200) {\n    json_content &lt;- content(response, as = \"text\", encoding = \"UTF-8\")\n    parsed_data &lt;- fromJSON(json_content, flatten = TRUE)\n    \n    historical_data &lt;- parsed_data$data$tradesTable$rows\n    \n    data_frame &lt;- as.data.frame(historical_data)\n    \n    write_csv(data_frame, csv_path)\n    \n    print(paste(\"Data successfully saved to\", csv_path))\n  } else {\n    print(paste(\"Failed to fetch data. HTTP Status Code:\", status_code(response)))\n  }\n}\n\n\n###Finance Sector\nFetching data\n\n\nCode\n# bank\n#BAC_history_data_url &lt;- \"https://api.nasdaq.com/api/quote/BAC/historical?assetclass=stocks&fromdate=2019-11-20&limit=9999&todate=2024-11-20&random=27\"\nBAC_csv_path &lt;- \"./Datasets/Banking/BAC Historical Data.csv\"\n#fetch_and_save_data(BAC_history_data_url, BAC_csv_path)\n\n# GS_history_data_url &lt;- \"https://api.nasdaq.com/api/quote/GS/historical?assetclass=stocks&fromdate=2019-11-20&limit=9999&todate=2024-11-20&random=26\"\nGS_csv_path &lt;- \"./Datasets/Banking/GS Historical Data.csv\"\n# fetch_and_save_data(GS_history_data_url, GS_csv_path)\n\n# JPM_history_data_url &lt;- \"https://api.nasdaq.com/api/quote/JPM/historical?assetclass=stocks&fromdate=2019-11-20&limit=9999&todate=2024-11-20&random=29\"\nJPM_csv_path &lt;- \"./Datasets/Banking/JPM Historical Data.csv\"\n# fetch_and_save_data(JPM_history_data_url, JPM_csv_path)\n\n\n# MS_history_data_url &lt;- \"https://api.nasdaq.com/api/quote/MS/historical?assetclass=stocks&fromdate=2019-11-20&limit=9999&todate=2024-11-20&random=15\"\nMS_csv_path &lt;- \"./Datasets/Banking/MS Historical Data.csv\"\n# fetch_and_save_data(MS_history_data_url, MS_csv_path)\n\n\n# WFC_history_data_url &lt;- \"https://api.nasdaq.com/api/quote/MS/historical?assetclass=stocks&fromdate=2019-11-20&limit=9999&todate=2024-11-20&random=15\"\nWFC_csv_path &lt;- \"./Datasets/Banking/MS Historical Data.csv\"\n# fetch_and_save_data(WFC_history_data_url, WFC_csv_path)\n\n\nReading data:\n\n\nCode\nBAC &lt;- read_csv(BAC_csv_path)\n\n\nRows: 1259 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): date, close, open, high, low\nnum (1): volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nGS &lt;- read_csv(GS_csv_path)\n\n\nRows: 1259 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): date, close, open, high, low\nnum (1): volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nJPM &lt;- read_csv(JPM_csv_path)\n\n\nRows: 1259 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): date, close, open, high, low\nnum (1): volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nMS &lt;- read_csv(MS_csv_path)\n\n\nRows: 1259 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): date, close, open, high, low\nnum (1): volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\nWFC &lt;- read_csv(WFC_csv_path)\n\n\nRows: 1259 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): date, close, open, high, low\nnum (1): volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nChecking Null Values:\n\n\nCode\nmissing_values &lt;- list(\n  \"BAC\" = colSums(is.na(BAC)),\n  \"GS\" = colSums(is.na(GS)),\n  \"JPM\" = colSums(is.na(JPM)),\n  \"MS\" = colSums(is.na(MS)),\n  \"WFC\" = colSums(is.na(WFC))\n)\n\nfor (name in names(missing_values)) {\n  cat(paste(\"Missing values in\", name, \"dataset:\\n\"))\n  print(missing_values[[name]])\n  cat(\"\\n\")\n}\n\n\nMissing values in BAC dataset:\n  date  close volume   open   high    low \n     0      0      0      0      0      0 \n\nMissing values in GS dataset:\n  date  close volume   open   high    low \n     0      0      0      0      0      0 \n\nMissing values in JPM dataset:\n  date  close volume   open   high    low \n     0      0      0      0      0      0 \n\nMissing values in MS dataset:\n  date  close volume   open   high    low \n     0      0      0      0      0      0 \n\nMissing values in WFC dataset:\n  date  close volume   open   high    low \n     0      0      0      0      0      0 \n\n\nVisualizing Missing Data:\n\n\nCode\nlibrary(ggplot2)\nlibrary(naniar)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(tidyr)\n\n\n\n\nCode\n# List of datasets\ndata_list &lt;- list(BAC = BAC, GS = GS, JPM = JPM, MS = MS, WFC = WFC)\n\n# Generate missing value visualizations\nfor (name in names(data_list)) {\n  cat(\"Visualizing missing data for\", name, \"\\n\")\n  \n  # Plot\n  print(\n    vis_miss(data_list[[name]]) +\n      ggtitle(paste(\"Missing Data Visualization for\", name))\n  )\n}\n\n\nVisualizing missing data for BAC \n\n\n\n\n\n\n\n\n\nVisualizing missing data for GS \n\n\n\n\n\n\n\n\n\nVisualizing missing data for JPM \n\n\n\n\n\n\n\n\n\nVisualizing missing data for MS \n\n\n\n\n\n\n\n\n\nVisualizing missing data for WFC \n\n\n\n\n\n\n\n\n\n\n\nCode\n# List of datasets\ndata_list &lt;- list(BAC = BAC, GS = GS, JPM = JPM, MS = MS, WFC = WFC)\n\n# Combine all datasets into one data frame with a 'Dataset' column indicating the dataset name\ncombined_data &lt;- bind_rows(lapply(names(data_list), function(dataset_name) {\n  data &lt;- data_list[[dataset_name]]\n  data$Dataset &lt;- dataset_name\n  return(data)\n}))\n\n# Convert all columns to character type\ncombined_data &lt;- combined_data |&gt;\n  mutate(across(everything(), as.character))\n\n# Pivot longer and add a column for missing data\ncombined_data_long &lt;- combined_data |&gt;\n  pivot_longer(cols = -Dataset, names_to = \"Column\", values_to = \"Value\") |&gt;\n  mutate(Missing = is.na(Value))\n\n# Plot the missing data across datasets\nggplot(combined_data_long, aes(x = Column, y = Dataset, fill = Missing)) +\n  geom_tile(color = \"white\") +\n  scale_fill_manual(values = c(\"TRUE\" = \"red\", \"FALSE\" = \"gray90\")) +\n  theme_minimal() +\n  ggtitle(\"Missing Data Visualization Across Datasets\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis labels for better readability\n\n\n\n\n\n\n\n\n\nFor the finance sector dataset, we have plotted two types of graphs:\nA missing value heatmap for each company, which shows that there are no missing values across any column for each company. A comparative heatmap of the five companies in the finance sector, which confirms that none of the columns in any of the five datasets have missing values.\n###Tech Sector\nFetching data\n\n\nCode\n#AAPL_history_data_url &lt;- \"https://api.nasdaq.com/api/quote/AAPL/historical?assetclass=stocks&fromdate=2019-11-20&limit=9999&todate=2024-11-20&random=21\"\nAAPL_csv_path &lt;- \"./Datasets/TECH/AAPL Historical data.csv\"\n#fetch_and_save_data(AAPL_history_data_url, AAPL_csv_path)\n\n\n#GOOG_history_data_url &lt;- \"https://api.nasdaq.com/api/quote/GOOG/historical?assetclass=stocks&fromdate=2019-11-20&limit=9999&todate=2024-11-20&random=32\"\nGOOG_csv_path &lt;- \"./Datasets/TECH/GOOG class C Historical data.csv\"\n#fetch_and_save_data(GOOG_history_data_url, GOOG_csv_path)\n\n# META_history_data_url &lt;- \"https://api.nasdaq.com/api/quote/META/historical?assetclass=stocks&fromdate=2019-11-20&limit=9999&todate=2024-11-20&random=13\"\nMETA_csv_path &lt;- \"./Datasets/TECH/GOOG class C Historical data.csv\"\n# fetch_and_save_data(GOOG_history_data_url, GOOG_csv_path)\n \n\n# MSFT_history_data_url &lt;- \"https://api.nasdaq.com/api/quote/META/historical?assetclass=stocks&fromdate=2019-11-20&limit=9999&todate=2024-11-20&random=13\"\nMSFT_csv_path &lt;- \"./Datasets/TECH/MSFT Historical data.csv\"\n# fetch_and_save_data(MSFT_history_data_url, MSFT_csv_path)\n\n# NVDA_history_data_url &lt;- \"https://api.nasdaq.com/api/quote/NVDA/historical?assetclass=stocks&fromdate=2019-11-20&limit=9999&todate=2024-11-20&random=49\"\nNVDA_csv_path &lt;- \"./Datasets/TECH/NVDA historical data.csv\"\n# fetch_and_save_data(NVDA_history_data_url, NVDA_csv_path)\n\n\nReading data\n\n\nCode\naapl = read_csv(AAPL_csv_path)\n\n\nRows: 1259 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): date, close, open, high, low\nnum (1): volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\ngoog = read_csv(GOOG_csv_path)\n\n\nRows: 1259 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): date, close, open, high, low\nnum (1): volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nmeta = read_csv(META_csv_path)\n\n\nRows: 1259 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): date, close, open, high, low\nnum (1): volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nmsft = read_csv(MSFT_csv_path)\n\n\nRows: 1259 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): date, close, open, high, low\nnum (1): volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nnvda = read_csv(NVDA_csv_path)\n\n\nRows: 1259 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): date, close, open, high, low\nnum (1): volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\n#missing data\ndatasets &lt;- list(\n  \"AAPL\" = aapl,\n  \"GOOG\" = goog,\n  \"META\" = meta,\n  \"MSFT\" = msft,\n  \"NVDA\" = nvda\n)\n\nfor (name in names(datasets)) {\n  cat(paste(\"Missing values in\", name, \"dataset:\\n\"))\n  print(colSums(is.na(datasets[[name]])))\n  cat(\"\\n\")\n}\n\n\nMissing values in AAPL dataset:\n  date  close volume   open   high    low \n     0      0      0      0      0      0 \n\nMissing values in GOOG dataset:\n  date  close volume   open   high    low \n     0      0      0      0      0      0 \n\nMissing values in META dataset:\n  date  close volume   open   high    low \n     0      0      0      0      0      0 \n\nMissing values in MSFT dataset:\n  date  close volume   open   high    low \n     0      0      0      0      0      0 \n\nMissing values in NVDA dataset:\n  date  close volume   open   high    low \n     0      0      0      0      0      0 \n\n\nCode\n#missing values visualisation\n#install.packages('naniar')\n#install.packages('VIM')\n#library(dplyr)\n#library(tidyverse)\n#library(ggplot2)\n#library(naniar)\n\n# Assuming the files are named file1, file2, ..., file5\naapl$Source &lt;- \"Apple\"\ngoog$Source &lt;- \"Google\"\nmeta$Source &lt;- \"Meta\"\nmsft$Source &lt;- \"Microsoft\"\nnvda$Source &lt;- \"Nvidia\"\n\n# Combine the datasets\nall_files &lt;- bind_rows(aapl, goog, meta, msft, nvda)\n\n# Reshape data for visualization\n# Create a missingness indicator for each column\nall_files &lt;- all_files  |&gt; \n  mutate(across(-Source, as.character))\n\n# Pivot the data\nmissing_long &lt;- all_files  |&gt; \n  pivot_longer(cols = -Source, names_to = \"Variable\", values_to = \"Value\")  |&gt; \n  mutate(Missing = is.na(Value))\nlibrary(ggplot2)\n\nggplot(missing_long, aes(x = Variable, y = Source, fill = Missing)) +\n  geom_tile(color = \"white\") +\n  scale_fill_manual(\n    values = c(\"TRUE\" = \"red\", \"FALSE\" = \"blue\"),\n    labels = c(\"TRUE\" = \"Missing\", \"FALSE\" = \"Not Missing\")\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"Missing Data Across Files\",\n    x = \"Variables\",\n    y = \"Files\",\n    fill = \"Missing?\"\n  ) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n\n\nThere are 2 of types visualizations we have done to depict the missing values in the data.One using regular ggplot and the other using the naniar package.As seen from the graphs, there are no missing values.This trend can be attributed to the fact that all stocks have a price and they are updated accordingly. To deal with historical data, NASDAQ would probably use multiple techniques like archival techniques and curation processes etc. Another widely known fact is that financial data is almost always very clean.\n###Healthcare and Pharma Sector\nFetch Data:\n\n\nCode\n# JNJ_history_data_url &lt;- \"https://api.nasdaq.com/api/quote/JNJ/historical?assetclass=stocks&fromdate=2019-11-20&limit=9999&todate=2024-11-20&random=45\"\nJNJ_csv_path &lt;- \"./Datasets/Healthcare and Pharma/JNJ Historical data.csv\"\n# fetch_and_save_data(JNJ_history_data_url, JNJ_csv_path)\n# \n# \n# AZN_history_data_url &lt;- \"https://api.nasdaq.com/api/quote/AZN/historical?assetclass=stocks&fromdate=2019-11-20&limit=9999&todate=2024-11-20&random=20\"\nAZN_csv_path &lt;- \"./Datasets/Healthcare and Pharma/AZN Historical data.csv\"\n# fetch_and_save_data(AZN_history_data_url, AZN_csv_path)\n# \n# \n# LLY_history_data_url &lt;- \"https://api.nasdaq.com/api/quote/LLY/historical?assetclass=stocks&fromdate=2019-11-20&limit=9999&todate=2024-11-20&random=22\"\nLLY_csv_path &lt;- \"./Datasets/Healthcare and Pharma/LLY Historical data.csv\"\n# fetch_and_save_data(LLY_history_data_url, LLY_csv_path)\n# \n# MRK_history_data_url &lt;- \"https://api.nasdaq.com/api/quote/MRK/historical?assetclass=stocks&fromdate=2019-11-20&limit=9999&todate=2024-11-20&random=33\"\nMRK_csv_path &lt;- \"./Datasets/Healthcare and Pharma/MRK Historical data.csv\"\n# fetch_and_save_data(MRK_history_data_url, MRK_csv_path)\n# \n# PFE_history_data_url &lt;- \"https://api.nasdaq.com/api/quote/PFE/historical?assetclass=stocks&fromdate=2019-11-20&limit=9999&todate=2024-11-20&random=49\"\nPFE_csv_path &lt;- \"./Datasets/Healthcare and Pharma/PFE Historical data.csv\"\n# fetch_and_save_data(PFE_history_data_url, PFE_csv_path)\n\n\nReading Data:\n\n\nCode\n# Reading the datasets using relative paths\njnj = read_csv(JNJ_csv_path)\n\n\nRows: 1259 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): date, close, open, high, low\nnum (1): volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nazn = read_csv(AZN_csv_path)\n\n\nRows: 1259 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): date, close, open, high, low\nnum (1): volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nlly = read_csv(LLY_csv_path)\n\n\nRows: 1259 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): date, close, open, high, low\nnum (1): volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nmrk = read_csv(MRK_csv_path)\n\n\nRows: 1259 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): date, close, open, high, low\nnum (1): volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\npfe = read_csv(PFE_csv_path)\n\n\nRows: 1259 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): date, close, open, high, low\nnum (1): volume\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCode\npharma_datasets &lt;- list(\n  \"JNJ\" = jnj,\n  \"AZN\" = azn,\n  \"LLY\" = lly,\n  \"MRK\" = mrk,\n  \"PFE\" = pfe\n)\n\nfor (name in names(pharma_datasets)) {\n  cat(paste(\"Missing values in\", name, \"dataset:\\n\"))\n  print(colSums(is.na(pharma_datasets[[name]])))\n  cat(\"\\n\")\n}\n\n\nMissing values in JNJ dataset:\n  date  close volume   open   high    low \n     0      0      0      0      0      0 \n\nMissing values in AZN dataset:\n  date  close volume   open   high    low \n     0      0      0      0      0      0 \n\nMissing values in LLY dataset:\n  date  close volume   open   high    low \n     0      0      0      0      0      0 \n\nMissing values in MRK dataset:\n  date  close volume   open   high    low \n     0      0      0      0      0      0 \n\nMissing values in PFE dataset:\n  date  close volume   open   high    low \n     0      0      0      0      0      0 \n\n\nMissing Values Visualisation\n\n\nCode\n# Assuming the files are named file1, file2, ..., file5\njnj$Source &lt;- \"JNJ\"\nazn$Source &lt;- \"AstraZeneca\"\nlly$Source &lt;- \"Eli Lilly\"\nmrk$Source &lt;- \"Merck&Co\"\npfe$Source &lt;- \"Pfizer\"\n\n# Combine the datasets\nall_files_health &lt;- bind_rows(jnj, azn, lly, mrk, pfe)\n\n# Reshape data for visualization\n# Create a missingness indicator for each column\nall_files_health &lt;- all_files_health  |&gt; \n  mutate(across(-Source, as.character))\n\n# Pivot the data\nmissing_long &lt;- all_files_health  |&gt; \n  pivot_longer(cols = -Source, names_to = \"Variable\", values_to = \"Value\")  |&gt; \n  mutate(Missing = is.na(Value))\nlibrary(ggplot2)\nlibrary(naniar)\n\ngg_miss_var(all_files_health, facet = Source) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  labs(title = \"Missing Data Visualization by File\")\n\n\n\n\n\n\n\n\n\nWe have used naniar package to visualise missing data. The x-axis shows the proportion of data missing in the dataset and here since its 0, it means there are no missing values. The reason why has been explained above.\nThere are 3 of types visualizations we have done to depict the missing values in the data. Two using regular ggplot and the other using the naniar package. As seen from the graphs, there are no missing values.This trend can be attributed to the fact that all stocks have a price and they are updated accordingly. To deal with historical data, NASDAQ would probably use multiple techniques like archival techniques and curation processes etc. Another widely known fact is that financial data is almost always very clean.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data</span>"
    ]
  },
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "3  Results",
    "section": "",
    "text": "4 Results",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#executive-summary",
    "href": "results.html#executive-summary",
    "title": "3  Results",
    "section": "4.1 Executive Summary",
    "text": "4.1 Executive Summary\nThis section presents the key findings from our sector-based financial analysis of Technology, Healthcare, and Banking stocks listed on NASDAQ over the past five years. Our analysis reveals distinct performance patterns, risk profiles, and sensitivities to economic indicators across the sectors. Notably, the Technology sector demonstrated robust growth and resilience, while the Banking sector exhibited higher volatility influenced by interest rate fluctuations. This project aims to explore the performance of stocks across 3 sectors and find out which stock is a safer bet i.e, less risk but there is also less profit and which one will generate more profit. We would like to establish the fact that this project does not aim to provide financial advice. To obtain the goal, we have to start somewhere. So, first, we have analysed the closing price and trading volume of each stock in each sector. The dataset spans over 5 years from 2019 to 2020.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#stock-performances-on-each-sector",
    "href": "results.html#stock-performances-on-each-sector",
    "title": "3  Results",
    "section": "4.2 1. Stock Performances on each sector",
    "text": "4.2 1. Stock Performances on each sector\n\n4.2.1 1.1 Closing Price\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\nlibrary(readr)\nlibrary(tidyr)\nlibrary(lubridate)\n\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nCode\nlibrary(zoo)  \n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nCode\nbanking_files &lt;- list.files(path = \"./Datasets/Banking/\", pattern = \"*.csv\", full.names = TRUE)\nbanking_files &lt;- banking_files[!grepl(\"_modified\", banking_files)]\n\ntech_files &lt;- list.files(path = \"./Datasets/TECH/\", pattern = \"*.csv\", full.names = TRUE)\ntech_files &lt;- tech_files[!grepl(\"_modified\", tech_files)]\n\nhealth_files &lt;- list.files(path = \"./Datasets/Healthcare and Pharma/\", pattern = \"*.csv\", full.names = TRUE)\nhealth_files &lt;- health_files[!grepl(\"_modified\", health_files)]\n\nsave_to_csv &lt;- function(data, file_path) {\n  write_csv(data, file_path)\n}\nstock_file_mapping &lt;- list()\n\nread_sector_data &lt;- function(file_path, sector) {\n  data &lt;- read_csv(file_path, show_col_types = FALSE)  \n  \n  colnames(data) &lt;- tolower(gsub(\"\\\\s+\", \"_\", colnames(data)))  \n  \n  required_columns &lt;- c(\"date\", \"close\", \"open\", \"high\", \"low\")\n  missing_columns &lt;- setdiff(required_columns, colnames(data))\n  if (length(missing_columns) &gt; 0) {\n    stop(paste(\"Missing required columns in file:\", file_path, \n               \"-&gt; Missing columns:\", paste(missing_columns, collapse = \", \")))\n  }\n  \n  data &lt;- data |&gt;\n    mutate(\n      date = mdy(date),  \n      close = as.numeric(gsub(\"\\\\$\", \"\", close)),  \n      open = as.numeric(gsub(\"\\\\$\", \"\", open)),\n      high = as.numeric(gsub(\"\\\\$\", \"\", high)),\n      low = as.numeric(gsub(\"\\\\$\", \"\", low)),\n      volume = as.numeric(gsub(\",\", \"\", volume))  \n    ) |&gt;\n    mutate(\n      Sector = sector,\n      Source = tools::file_path_sans_ext(basename(file_path))\n    )\n  \n  stock_file_mapping[[unique(data$Source)]] &lt;&lt;- file_path\n  \n  return(data)\n}\n\nbanking_data &lt;- bind_rows(lapply(banking_files, read_sector_data, sector = \"Banking\"))\ntech_data &lt;- bind_rows(lapply(tech_files, read_sector_data, sector = \"Technology\"))\nhealth_data &lt;- bind_rows(lapply(health_files, read_sector_data, sector = \"Healthcare\"))\n\ncombined_data &lt;- bind_rows(banking_data, tech_data, health_data)\n\ncombined_data &lt;- combined_data |&gt;\n  rename(Stock = Source)\n\np &lt;- ggplot(combined_data, aes(x = date, y = close, color = Stock)) +\n  geom_line(size = 1) +\n  facet_grid(Sector ~ ., scales = \"free_y\") +  \n  labs(\n    title = \"Stock Closing Prices Faceted by Sector\",\n    x = \"Date\",\n    y = \"Closing Price (USD)\",\n    color = \"Stock\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"bottom\"\n  )\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nCode\nprint(p)\n\n\n\n\n\n\n\n\n\nTech stocks are showing pronounced growth over time due to the investors’ belief of growth in technology. Particularly for Nvidia, there is a significant increase. In healthcare, Eli Lilly is exhibiting a steady growth although not as steep as Technology. There is a slower but stable upward trend in the banking sector. This could be attributed to various reasons. One being the restrictions and regulations that the banking sectors must adhere to which limits their rapid expansion. The other one might be their focus on mature markets rather than younger ones. Unlike Tech and some healthcare companies, the banks look for stability over innovation. They might require stable economic growth and there are very few growth opportunities in a mature market. We have stated a few reasons as to why the slower increase is seen, but there are various other reasons as well.\nNow, let’s move on to Opening prices. Opening price is the first trading price of a stock on a particular date. We did plot a graph for it but we haven’t included it because it yields a similar conclusion as closing price. This could be because of broader market conditions or sector-specific factors as well. One such reason is the Efficient Market Hypothesis(EMH) which states that share prices reflect all the information available. Meaning, all the changes that happened overnight when the market was closed is being reflected in the opening price for the next day. So, there is little to improve throughout the day unless something huge happens in the middle of the day for the market to crash.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#trade-volume",
    "href": "results.html#trade-volume",
    "title": "3  Results",
    "section": "4.3 Trade Volume",
    "text": "4.3 Trade Volume\nMoving on to the next phase of our analysis: Trading Volumes. This step is crucial as it sets the tone for our subsequent analysis focused on events.\n\n\nCode\np &lt;- ggplot(combined_data, aes(x = date, y = volume, color = Stock)) +\n  geom_line(size = 1) +\n  facet_grid(Sector ~ ., scales = \"free_y\") +  \n  labs(\n    title = \"Stock Trading Volume Faceted by Sector\",\n    x = \"Date\",\n    y = \"Volume\",\n    color = \"Stock\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    legend.position = \"bottom\"\n  )\n\nprint(p)\n\n\n\n\n\n\n\n\n\nWe see some interesting patterns here. It highlights investor patterns and market interests. There are significant spikes in the volume of META and NVDA, this indicates heightened investor interests during certain periods. We will specifically focus on NVDA because we are seeing very intriguing trends. Nvidia was always a leader in the GPU market. With its integration of AI, it has set the industry standard. After the hit of COVID-19, companies announced work-from-home policies, and educational institutions shifted to online learning. There was huge demand for PCs. This meant, GPUs. From Source),), we find that Nvidia increased their GPU shares from 69.19% in Q1 to 80% in Q2. This indicates the surges in their trading volume in 2020.Then came the AI boom, driving Nvidia stocks to an all-time high. Now it is considered to be one of the world’s richest countries.\nYet again, the banking sector shows a smaller growth. This could be attributed to major geopolitical events and the pandemic. Since banking sectors look for economic stability more, they are one of the first ones to get hit. Therefore, there isn’t much growth in their trading volume.\nWhen it comes to healthcare, AZN, PFE and MRK display smaller growths, but ELY and JNJ show major peaks in 2020 and 2022. In 2020, it could be attributed to the demand for vaccines and ELY and JNJ are in the forefront of healthcare and medtech. This is to be expected in 2020 because while other sectors are facing losses, healthcare benefits the most because of the demand for vaccines and medicines.\nTo summarise: Tech - Higher closing price and trading volumes indicates their volatility yet higher-growth nature. Healthcare - Steady rise in stock prices indicate investor’s confidence with occasional volume surges based on specific events Banking - Conservative price changes and volume indicate the sector’s stable nature.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#stock-analysis-by-events",
    "href": "results.html#stock-analysis-by-events",
    "title": "3  Results",
    "section": "4.4 2. Stock Analysis By Events",
    "text": "4.4 2. Stock Analysis By Events\nNow, let’s focus on how each event has impacted the stocks. We have plotted an interactive graph for each sector using Plotly so that the users can select which period to focus on. We have excluded the banking sector because it has already been established that whenever there is any economical instability, there won’t be much growth in the banking sector. Unless it is as interesting as the credit crisis, we felt that it isn’t worth showing and would turn out to be redundant for our end goal. We are focusing on 7 major events: 2019-04-15 - Notre-Dame Fire 2020-03-11 - Covid-19 declared 2020-12-14 - Vaccine rollout begins 2021-01-06 - Capitol Riot 2022-02-24 - Russia Invades Ukraine 2023-10-07 - Israel-Hamas Conflict 2024-11-05 - 2025 Presidential Election\n\n4.4.1 2.1 Healthcare\n\n\nCode\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(plotly)\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nCode\nlibrary(zoo)\n\n# Extract stock names from the Source column and pivot the data\nwide_data &lt;- health_data |&gt; \n  mutate(Stock = sub(\" Historical data\", \"\", Source)) |&gt; \n  select(date, close, Stock) |&gt; \n  pivot_wider(names_from = Stock, values_from = close) |&gt; \n  arrange(date)\n\n# Fill missing values using linear interpolation\nwide_data &lt;- wide_data |&gt; \n  mutate(across(-date, ~ na.approx(.x, na.rm = FALSE)))\n\nevents &lt;- data.frame(\n  date = as.Date(c(\"2019-04-15\", \"2020-03-11\", \"2020-12-14\", \"2021-01-06\", \n                   \"2022-02-24\", \"2023-10-07\", \"2024-11-05\")),\n  event = c(\"Notre-Dame Fire\", \"COVID-19 Declared\", \"Vaccine Rollout Begins\", \n            \"Capitol Riot\", \"Russia Invades Ukraine\", \"Israel-Hamas Conflict\", \n            \"2024 Presidential Election\"),\n  stringsAsFactors = FALSE\n)\n\nanalyze_event_impact &lt;- function(stock_data, event_date) {\n  event_date &lt;- as.Date(event_date)\n  pre_event_date &lt;- event_date - days(3)\n  post_event_date &lt;- event_date + days(3)\n  \n  relevant_prices &lt;- stock_data |&gt; \n    filter(date &gt;= pre_event_date & date &lt;= post_event_date)\n  \n  stock_columns &lt;- setdiff(names(relevant_prices), \"date\")\n  \n  returns &lt;- relevant_prices |&gt; \n    mutate(across(all_of(stock_columns), ~ (.x / lag(.x) - 1) * 100, .names = \"{.col}_return\")) |&gt;\n    mutate(event_date = event_date)\n  \n  return(returns)\n}\n\nimpact_results &lt;- events |&gt; \n  rowwise() |&gt; \n  do(analyze_event_impact(wide_data, .$date)) |&gt; \n  ungroup()\n\nresults_df &lt;- bind_rows(impact_results)\n\n# Remove rows with all NA values in return columns\nresults_df &lt;- results_df |&gt; \n  filter(if_any(ends_with(\"_return\"), ~ !is.na(.)))\n\n# Prepare data for plotting\nplot_data &lt;- results_df |&gt; \n  pivot_longer(\n    cols = ends_with(\"_return\"), \n    names_to = \"Stock\", \n    values_to = \"Return\",\n    names_pattern = \"(.+)_return\"\n  ) |&gt; \n  filter(!is.na(Return))\n\nplot &lt;- plot_ly()\n\n# Add a trace for each stock\nstocks &lt;- unique(plot_data$Stock) # Get unique stock names from plot_data\nfor (stock in stocks) {\n  stock_data &lt;- plot_data |&gt; filter(Stock == stock)\n  \n  plot &lt;- plot |&gt; add_trace(\n    type = 'scatter',\n    mode = 'lines+markers',\n    x = stock_data$date,\n    y = stock_data$Return,\n    name = stock,\n    legendgroup = stock,\n    visible = 'legendonly' # Start with traces hidden\n  )\n}\n\nprint(stocks)\n\n\n[1] \"AZN Historical Data\" \"JNJ\"                 \"LLY Historical Data\"\n[4] \"MRK Historical Data\" \"PFE Historical Data\"\n\n\nCode\n# Add vertical lines for events\nfor (i in seq_along(events$date)) {\n  plot &lt;- plot |&gt; add_segments(\n    x = events$date[i], xend = events$date[i],\n    y = min(plot_data$Return, na.rm = TRUE), yend = max(plot_data$Return, na.rm = TRUE),\n    line = list(color = 'red', dash='dash'),\n    showlegend = FALSE\n  )\n}\n\n# Customize layout\nplot &lt;- plot |&gt; layout(\n  title = \"Stock Price Changes Around Major Events\",\n  xaxis = list(title = \"Date\"),\n  yaxis = list(title = \"Percentage Change (%)\"),\n  hovermode = \"x unified\"\n)\n\n# Display the plot\nplot\n\n\n\n\n\n\nThe graph illustrates the stock price percentage changes for five pharmaceutical companies—AstraZeneca (AZN), Johnson & Johnson (JNJ), Eli Lilly (LLY), Merck (MRK), and Pfizer (PFE)—around major events marked by red dashed vertical lines. All the companies experienced high volatility between 2020 and 2021, primarily due to events like Covid-19. In 2021, most companies showed significant price drops, with AZN experiencing the most substantial dip, while MRK didn’t experience a sharp decrease. AZN shows the highest level of volatility, with notable dips in stock prices followed by recoveries and dropping again between 2024 and 2025. JNJ and MRK also experienced high fluctuation between 2020 and 2021 but recovered in the coming years.\n\n\n4.4.2 2.2 Technology\n\n\nCode\n# Extract stock names from the Source column and pivot the data\nwide_data &lt;- tech_data |&gt; \n  mutate(Stock = sub(\" Historical data\", \"\", Source)) |&gt; \n  select(date, close, Stock) |&gt; \n  pivot_wider(names_from = Stock, values_from = close) |&gt; \n  arrange(date)\n\n# Fill missing values using linear interpolation\nwide_data &lt;- wide_data |&gt; \n  mutate(across(-date, ~ na.approx(.x, na.rm = FALSE)))\n\n# Apply the function to each event and combine the results\nimpact_results &lt;- events |&gt; \n  rowwise() |&gt; \n  do(analyze_event_impact(wide_data, .$date)) |&gt; \n  ungroup()\n\n# Combine results into a single data frame for reporting\nresults_df &lt;- bind_rows(impact_results)\n\n# Remove rows with all NA values in return columns\nresults_df &lt;- results_df |&gt; \n  filter(if_any(ends_with(\"_return\"), ~ !is.na(.)))\n\n# Prepare data for plotting\nplot_data &lt;- results_df |&gt; \n  pivot_longer(\n    cols = ends_with(\"_return\"), \n    names_to = \"Stock\", \n    values_to = \"Return\",\n    names_pattern = \"(.+)_return\"\n  ) |&gt; \n  filter(!is.na(Return))\n\n# Create interactive plot using plotly\nplot &lt;- plot_ly()\n\n# Add a trace for each stock\nstocks &lt;- unique(plot_data$Stock) # Get unique stock names from plot_data\nfor (stock in stocks) {\n  stock_data &lt;- plot_data |&gt; filter(Stock == stock)\n  \n  plot &lt;- plot |&gt; add_trace(\n    type = 'scatter',\n    mode = 'lines+markers',\n    x = stock_data$date,\n    y = stock_data$Return,\n    name = stock,\n    legendgroup = stock,\n    visible = 'legendonly' # Start with traces hidden\n  )\n}\n\n# Add vertical lines for events\nfor (i in seq_along(events$date)) {\n  plot &lt;- plot |&gt; add_segments(\n    x = events$date[i], xend = events$date[i],\n    y = min(plot_data$Return, na.rm = TRUE), yend = max(plot_data$Return, na.rm = TRUE),\n    line = list(color = 'red', dash='dash'),\n    showlegend = FALSE\n  )\n}\n\n# Customize layout\nplot &lt;- plot |&gt; layout(\n  title = \"Stock Price Changes Around Major Events\",\n  xaxis = list(title = \"Date\"),\n  yaxis = list(title = \"Percentage Change (%)\"),\n  hovermode = \"x unified\"\n)\n\n# Display the plot\nplot\n\n\n\n\n\n\nTrends observed: ~2019 The tech stocks seem unaffected by the notre-dame fire, as it likely had no significant impact on technology.\n~2020 A sharp decline in stock prices across all companies indicating panic and uncertainty in the early stages of the pandemic. The tech industry struggled initially. That is why a sharp decrease in percentage change is observed. But, once remote work and digital transformation took place, investors felt that tech might thrive under these circumstances. This is shown by a sudden increase. Companies like Nvidia and Microsoft saw increased demand. This sudden spike and fluctuation is due to market sentiments and the reevaluation of the resilience and growth opportunities that the tech industry has. A noticeable upward trend is visible from vaccine rollout.\n~2021 Significant drops around 2021 are observed suggesting market instability and it has started to negatively impact the stocks. Google and Microsoft seem more resilient with less extreme declines. But they recovered almost immediately. According to major media sites like Forbes, ‘The stock market does not care about violence and riots’.\n~2022 Once more, we see a noticeable dip due to Russia invading Ukraine. This reflects global uncertainty and geopolitical tensions. But the stocks appear to recover quickly. Global demand for tech services like AI applications, digital services, cloud computing etc. kept the industry afloat and it could also be because of limited direct exposure to eastern european countries like Russia and Ukraine. But this does not exclude the fact that there were major supply-chain disruptions. But again, the intensity seems to be muted compared to COVID-19 or the GFC.\n~2023 The Israel-Hamas War did not set off market panic. There is minimal impact on tech stock performance. After viewing multiple sources(CNN, CTech, Science Direct), we have concluded that Israeli Tech stocks got hit severely and there wasn’t much impact on international companies like Nvidia. We do observe that Apple, Google, Microsoft and Meta had experienced significant decrease and increase. The only major concern of investors and other financial analysts was that it could send the economy into a recession. Only in that case would there be a direct impact.\n~2024 Towards 2024, there is a gradual positive trend with NVDA and META, showing a slightly higher recovery rate compared to the others. This could be fueled by optimism or expectation towards new leadership.\nTherefore, there are varying levels of impact on tech stocks due to various major events that happened. But the overall trend is that the market always recovers. Slowly, but surely.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#volatility-analysis",
    "href": "results.html#volatility-analysis",
    "title": "3  Results",
    "section": "4.5 3 Volatility Analysis",
    "text": "4.5 3 Volatility Analysis\nLet’s revisit our goal for this whole project. We want to find out which stock will return more profits and which one returns less profit but is a safer bet. It is crucial to understand that more risk implies more profit. But not many investors would want to take a gamble and risk losing their money. In that case, what can they do? They would need to analyse what stocks have little to no risk associated with them. But, keep in mind that they might generate less profit as well. There are some key metrics to evaluate risk and return such as, RSI(Relative Strength Index), Sharpe Ratio, Drawdown, Rolling Volatility etc. Let’s understand what each of these terms mean, RSI - It indicates the overbought and oversold conditions for a stock to help traders make informed decisions. If RSI is over 70, it means that it is overbought and if it is below 30, it is oversold. RSI is basically a momentum indicator that provides short-term buy or sell signals. Drawdown - It is a peak to trough decline during a particular period. It measures risk and downside volatility. Sharpe Ratio - Rolling Volatility -\nIn our analysis, we have focused on Rolling Volatility, Sharpe Ratio and Drawdown. We did do RSI but we realised that it would just show oversold and overbought conditions and not really do much to our end goal so, we have not presented it in our final output. We have considered Drawdown and Sharpe Ratio for our analysis since they directly associate with what we are trying to accomplish.\n\n4.5.1 3.1 Healthcare\n\n\nCode\n# Install and load required packages\n# Install and load required packages\n#install.packages(\"slider\")\nlibrary(slider)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Calculate daily returns\nhealth_data &lt;- health_data |&gt;\n  group_by(Source) |&gt;\n  arrange(date) |&gt;\n  mutate(daily_return = (close / lag(close) - 1) * 100) |&gt;\n  ungroup()\n\n# Calculate volatility (20-day rolling standard deviation of returns)\nhealth_data &lt;- health_data |&gt;\n  group_by(Source) |&gt;\n  mutate(volatility = slide_dbl(daily_return, sd, .before = 19, .complete = TRUE)) |&gt;\n  ungroup()\n\n# Calculate average returns\navg_returns &lt;- health_data |&gt;\n  group_by(Source) |&gt;\n  summarize(avg_daily_return = mean(daily_return, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_daily_return))\n\nprint(avg_returns)\n\n\n# A tibble: 5 × 2\n  Source              avg_daily_return\n  &lt;chr&gt;                          &lt;dbl&gt;\n1 LLY Historical Data           0.170 \n2 AZN Historical Data           0.0373\n3 MRK Historical Data           0.0243\n4 JNJ Historical data           0.0171\n5 PFE Historical Data          -0.0148\n\n\nCode\nhealth_data_clean &lt;- health_data |&gt; filter(!is.na(daily_return))\n\n# Plot daily returns\nggplot(health_data_clean, aes(x = date, y = daily_return, color = Source)) +\n  geom_line() +\n  labs(title = \"Daily Returns by Stock\", x = \"Date\", y = \"Daily Return (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe ‘Daily Returns by Stock’ graph illustrates the daily returns of pharmaceutical stocks (AZN, JNJ, LLY, MRK, and PFE) over time. More spikes from 2020 to 2022 indicate a significant market event affecting the return of the stocks, such as the COVID-19 pandemic in early 2020. While the stock returns mostly stay close to zero, they seem to be affected by specific events like product launches, regulatory approvals, or market-wide shocks, which can be a factor for the fluctuations.\n\n\nCode\n# Plot volatility\nhealth_data_clean &lt;- health_data |&gt; filter(!is.na(volatility))\n# ggplot(health_data_clean, aes(x = date, y = volatility, color = Source)) +\n#   geom_line() +\n#   labs(title = \"Volatility (20-Day Rolling Standard Deviation) by Stock\", x = \"Date\", y = \"Volatility (%)\") +\n#   theme_minimal()\n\n\n\n\nCode\nlibrary(TTR)\n\nhealth_data &lt;- health_data |&gt;\n  group_by(Source) |&gt;\n  mutate(RSI = TTR::RSI(close, n = 14)) |&gt;\n  ungroup()\n\nhealth_data &lt;- health_data |&gt; filter(!is.na(RSI))\n# \n# create_rsi_plot &lt;- function(data, stock_name) {\n#   stock_data &lt;- data |&gt; filter(Source == stock_name)\n#   \n#   ggplot(stock_data, aes(x = date)) +\n#     geom_rect(aes(xmin = min(date), xmax = max(date), ymin = 70, ymax = 100), \n#               fill = \"pink\", alpha = 0.2) +\n#     geom_rect(aes(xmin = min(date), xmax = max(date), ymin = 0, ymax = 30), \n#               fill = \"lightgreen\", alpha = 0.2) +\n#     geom_line(aes(y = RSI), color = \"black\", linewidth = 0.8) +\n#     geom_hline(yintercept = 70, color = \"red\", linetype = \"dashed\", linewidth = 0.8) +\n#     geom_hline(yintercept = 30, color = \"green\", linetype = \"dashed\", linewidth = 0.8) +\n#     geom_hline(yintercept = 50, color = \"gray\", linetype = \"dotted\") +\n#     labs(title = paste(\"RSI Over Time -\", stock_name), \n#          subtitle = \"With Overbought (&gt;70) and Oversold (&lt;30) Zones\",\n#          x = \"Date\", \n#          y = \"RSI\") +\n#     scale_y_continuous(limits = c(0, 100)) +\n#     theme_minimal() +\n#     theme(\n#       panel.grid.minor = element_blank(),\n#       axis.text.x = element_text(angle = 45, hjust = 1),\n#       plot.title = element_text(face = \"bold\"),\n#       plot.subtitle = element_text(size = 9)\n#     )\n# }\n\n# # Create plots for each stock\n# stock_names &lt;- unique(health_data$Source)\n# for(stock in stock_names) {\n#   print(create_rsi_plot(health_data, stock))\n# }\n\n\n\n\nCode\nlibrary(gridExtra)\n\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\n\nCode\nlibrary(grid)\nperformance_metrics &lt;- health_data |&gt;\n  group_by(Source) |&gt;\n  summarize(\n    sharpe_ratio = mean(daily_return, na.rm = TRUE) / \n                  sd(daily_return, na.rm = TRUE) * sqrt(252),\n    max_drawdown = min(cumsum(daily_return), na.rm = TRUE),\n    avg_volatility = mean(volatility, na.rm = TRUE)\n  ) |&gt;\n  arrange(desc(sharpe_ratio))\n\nprint(performance_metrics)\n\n\n# A tibble: 5 × 4\n  Source              sharpe_ratio max_drawdown avg_volatility\n  &lt;chr&gt;                      &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1 LLY Historical Data        1.30         0.108           1.86\n2 AZN Historical Data        0.344      -21.5             1.54\n3 MRK Historical Data        0.225      -27.3             1.34\n4 JNJ Historical data        0.190      -20.7             1.08\n5 PFE Historical Data       -0.165      -27.5             1.59\n\n\nCode\n# Calculate rolling metrics\n\n  health_data &lt;- health_data |&gt; group_by(Source) |&gt; \n  arrange(date) |&gt;\n  mutate(\n    # Rolling Sharpe Ratio (252-day window)\n    rolling_returns_mean = zoo::rollmean(daily_return, k = 252, fill = NA),\n    rolling_returns_sd = zoo::rollapply(daily_return, width = 252, FUN = sd, fill = NA),\n    rolling_sharpe = (rolling_returns_mean / rolling_returns_sd) * sqrt(252),\n    \n    # Rolling Maximum Drawdown\n    cumulative_return = cumprod(1 + daily_return),\n    rolling_max = zoo::rollmax(cumulative_return, k = 252, fill = NA),\n    rolling_drawdown = (cumulative_return - rolling_max) / rolling_max\n  ) |&gt;\n  ungroup()\n\nhealth_data &lt;- health_data |&gt; filter(!is.na(rolling_sharpe))\nhealth_data &lt;- health_data |&gt; filter(!is.na(rolling_drawdown))\n\n\n# Create rolling Sharpe ratio plot\nsharpe_plot &lt;- ggplot(health_data, aes(x = date, y = rolling_sharpe, color = Source)) +\n  geom_line() +\n  labs(title = \"Rolling Sharpe Ratio Over Time (252-day window)\",\n       x = \"Date\", y = \"Sharpe Ratio\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n# Arrange plots vertically\nggplot(health_data, aes(x = date, y = rolling_sharpe, color = Source)) +\n  geom_line() +\n  labs(title = \"Rolling Sharpe Ratio Over Time\",\n       x = \"Date\", y = \"Sharpe Ratio\") +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nThe ‘Rolling Sharpe Ratio Over Time’ graph presents the rolling Sharpe ratios of five pharmaceutical companies [AstraZeneca (AZN), Johnson & Johnson (JNJ), Eli Lilly (LLY), Merck (MRK), and Pfizer (PFE)] over time. LLY has a higher Sharpe ratio throughout suggesting. high-risk adjust performance. Meanwhile, AZN and MRK have sharp declines followed by recoveries, indicating that they have more risk and inconsistent returns. PFE has a high decrease in the Sharpe ratio between 2023 and 2024, where its returns didn’t justify the risks taken. Overall, LLY offers the best performance, while the others vary in volatility, with PFE and JNJ showing more stability but less favorable returns than LLY.\n\n\nCode\n# Create rolling maximum drawdown plot\ndrawdown_plot &lt;- ggplot(health_data, aes(x = date, y = rolling_drawdown, color = Source)) +\n  geom_line() +\n  labs(title = \"Rolling Maximum Drawdown Over Time (252-day window)\",\n       x = \"Date\", y = \"Drawdown\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nggplot(health_data, aes(x = date, y = rolling_drawdown)) +\n  geom_line() +\n  labs(title = \"Rolling Maximum Drawdown Over Time\",\n       x = \"Date\", y = \"Drawdown\") +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  ) +\n  facet_wrap(~ Source)\n\n\n\n\n\n\n\n\n\nThe graph shows the rolling maximum drawdown over time for historical data from five pharmaceutical companies. AZN, JNJ, LLY, and PFE displayed stability with minimal drawdowns, indicating consistent performance and a low risk of losses during the observed period. However, MRK stands out due to a sharp drawdown in 2023, suggesting a significant decline in its stock value during 2023, which could be because of specific events or financial difficulties.\n\n\n4.5.2 3.2 Finance\n\n\nCode\n# Install and load required packages\n# Install and load required packages\n#install.packages(\"slider\")\nlibrary(slider)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Calculate daily returns\nbanking_data &lt;- banking_data |&gt;\n  group_by(Source) |&gt;\n  arrange(date) |&gt;\n  mutate(daily_return = (close / lag(close) - 1) * 100) |&gt;\n  ungroup()\n\n# Calculate volatility (20-day rolling standard deviation of returns)\nbanking_data &lt;- banking_data |&gt;\n  group_by(Source) |&gt;\n  mutate(volatility = slide_dbl(daily_return, sd, .before = 19, .complete = TRUE)) |&gt;\n  ungroup()\n\n# Calculate average returns\navg_returns &lt;- banking_data |&gt;\n  group_by(Source) |&gt;\n  summarize(avg_daily_return = mean(daily_return, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_daily_return))\n\nprint(avg_returns)\n\n\n# A tibble: 5 × 2\n  Source              avg_daily_return\n  &lt;chr&gt;                          &lt;dbl&gt;\n1 MS Historical Data            0.103 \n2 GS Historical Data            0.0998\n3 JPM Historical Data           0.0701\n4 WFC Historical Data           0.0557\n5 BAC Historical Data           0.0525\n\n\nCode\nbanking_data_clean &lt;- banking_data |&gt; filter(!is.na(daily_return))\n\n# Plot daily returns\nggplot(banking_data_clean, aes(x = date, y = daily_return, color = Source)) +\n  geom_line() +\n  labs(title = \"Daily Returns by Stock\", x = \"Date\", y = \"Daily Return (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nAs we can see, heavy volatility is seen at the start of the timeline in 2020 due to the world market’s reaction to the pandemic. After 2020, there is some stability but it is still fluctuating but has less volatility. All stocks seem to have overlapping patterns due to similar reactions to economical events.\n\n\nCode\n# Plot volatility\nbanking_data_clean &lt;- banking_data |&gt; filter(!is.na(volatility))\n# ggplot(banking_data_clean, aes(x = date, y = volatility, color = Source)) +\n#   geom_line() +\n#   labs(title = \"Volatility (20-Day Rolling Standard Deviation) by Stock\", x = \"Date\", y = \"Volatility (%)\") +\n#   theme_minimal()\n\n\n\n\nCode\nlibrary(TTR)\n\nbanking_data &lt;- banking_data |&gt;\n  group_by(Source) |&gt;\n  mutate(RSI = TTR::RSI(close, n = 14)) |&gt;\n  ungroup()\n\nbanking_data &lt;- banking_data |&gt; filter(!is.na(RSI))\n\n# create_rsi_plot &lt;- function(data, stock_name) {\n#   stock_data &lt;- data |&gt; filter(Source == stock_name)\n# \n#   ggplot(stock_data, aes(x = date)) +\n#     geom_rect(aes(xmin = min(date), xmax = max(date), ymin = 70, ymax = 100),\n#               fill = \"pink\", alpha = 0.2) +\n#     geom_rect(aes(xmin = min(date), xmax = max(date), ymin = 0, ymax = 30),\n#               fill = \"lightgreen\", alpha = 0.2) +\n#     geom_line(aes(y = RSI), color = \"black\", linewidth = 0.8) +\n#     geom_hline(yintercept = 70, color = \"red\", linetype = \"dashed\", linewidth = 0.8) +\n#     geom_hline(yintercept = 30, color = \"green\", linetype = \"dashed\", linewidth = 0.8) +\n#     geom_hline(yintercept = 50, color = \"gray\", linetype = \"dotted\") +\n#     labs(title = paste(\"RSI Over Time -\", stock_name),\n#          subtitle = \"With Overbought (&gt;70) and Oversold (&lt;30) Zones\",\n#          x = \"Date\",\n#          y = \"RSI\") +\n#     scale_y_continuous(limits = c(0, 100)) +\n#     theme_minimal() +\n#     theme(\n#       panel.grid.minor = element_blank(),\n#       axis.text.x = element_text(angle = 45, hjust = 1),\n#       plot.title = element_text(face = \"bold\"),\n#       plot.subtitle = element_text(size = 9)\n#     )\n# }\n\n# Create plots for each stock\n# stock_names &lt;- unique(banking_data$Source)\n# for(stock in stock_names) {\n#   print(create_rsi_plot(banking_data, stock))\n# }\n\n\n\n\nCode\nlibrary(gridExtra)\nlibrary(grid)\nperformance_metrics &lt;- banking_data |&gt;\n  group_by(Source) |&gt;\n  summarize(\n    sharpe_ratio = mean(daily_return, na.rm = TRUE) / \n                  sd(daily_return, na.rm = TRUE) * sqrt(252),\n    max_drawdown = min(cumsum(daily_return), na.rm = TRUE),\n    avg_volatility = mean(volatility, na.rm = TRUE)\n  ) |&gt;\n  arrange(desc(sharpe_ratio))\n\nprint(performance_metrics)\n\n\n# A tibble: 5 × 4\n  Source              sharpe_ratio max_drawdown avg_volatility\n  &lt;chr&gt;                      &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1 GS Historical Data         0.753        -43.4           1.82\n2 MS Historical Data         0.730        -50.4           1.94\n3 JPM Historical Data        0.524        -46.7           1.73\n4 BAC Historical Data        0.358        -54.7           1.95\n5 WFC Historical Data        0.358        -77.2           2.17\n\n\nCode\n# Calculate rolling metrics\nbanking_data &lt;- banking_data |&gt;\n  group_by(Source) |&gt;\n  arrange(date) |&gt;\n  mutate(\n    # Rolling Sharpe Ratio (252-day window)\n    rolling_returns_mean = zoo::rollmean(daily_return, k = 252, fill = NA),\n    rolling_returns_sd = zoo::rollapply(daily_return, width = 252, FUN = sd, fill = NA),\n    rolling_sharpe = (rolling_returns_mean / rolling_returns_sd) * sqrt(252),\n    \n    # Rolling Maximum Drawdown\n    cumulative_return = cumprod(1 + daily_return),\n    rolling_max = zoo::rollmax(cumulative_return, k = 252, fill = NA),\n    rolling_drawdown = (cumulative_return - rolling_max) / rolling_max\n  ) |&gt;\n  ungroup()\n\nbanking_data &lt;- banking_data |&gt; filter(!is.na(rolling_sharpe))\nbanking_data &lt;- banking_data |&gt; filter(!is.na(rolling_drawdown))\n\n\n# Create rolling Sharpe ratio plot\nsharpe_plot &lt;- ggplot(banking_data, aes(x = date, y = rolling_sharpe, color = Source)) +\n  geom_line() +\n  labs(title = \"Rolling Sharpe Ratio Over Time (252-day window)\",\n       x = \"Date\", y = \"Sharpe Ratio\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n# Arrange plots vertically\nggplot(banking_data, aes(x = date, y = rolling_sharpe, color = Source)) +\n  geom_line() +\n  labs(title = \"Rolling Sharpe Ratio Over Time\",\n       x = \"Date\", y = \"Sharpe Ratio\") +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nAround 2020-2021, the Sharpe ratio for all stocks spiked significantly. This indicates higher risk-adjusted returns due market rebound following the pandemic. The banking sector benefitted from government aid, economic recovery and trading revenues. However, it dipped again in 2022 due to inflation, interest rate changes, potential recession concerns and so on. From 2023, it has improved a lot and the recovery correlates with interest rate stabilization and improved market sentiments.But of all these stocks, WFC has the lower and more fluctuating Share Ratios. GS is also seen to have a higher volatility in Sharpe Ratio due the nature of its firm(it relies more on investment banking and trading).\n\n\nCode\n# Create rolling maximum drawdown plot\ndrawdown_plot &lt;- ggplot(banking_data, aes(x = date, y = rolling_drawdown, color = Source)) +\n  geom_line() +\n  labs(title = \"Rolling Maximum Drawdown Over Time (252-day window)\",\n       x = \"Date\", y = \"Drawdown\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nggplot(banking_data, aes(x = date, y = rolling_drawdown)) +\n  geom_line() +\n  labs(title = \"Rolling Maximum Drawdown Over Time\",\n       x = \"Date\", y = \"Drawdown\") +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  ) +\n  facet_wrap(~ Source)\n\n\n\n\n\n\n\n\n\nUnlike Tech(which we will see below), all stocks here show pronounced drawdowns and dips to -1, reflecting significant volatility. For some, it exceeds -2.0, which is very concerning. On taking a closer look, these drawdowns seem more pronounced in late 2022 to 2023. This could be because of interest rate changes, regulatory impacts etc. Since the banking sector is more prone to such events, this just proves our previous statement that major economic events impact banking sectors more. GS and WFC have the most extreme drawdowns, indicating higher susceptibility to large losses. Morgan Stanley seems relatively stable to these market conditions with fewer dips.\n\n\n4.5.3 3.3 Technology\n\n\nCode\nlibrary(slider)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Calculate daily returns\ntech_data &lt;- tech_data |&gt;\n  group_by(Source) |&gt;\n  arrange(date) |&gt;\n  mutate(daily_returns = (close / lag(close) - 1) * 100) |&gt;\n  ungroup()\n\n# Calculate volatility (20-day rolling standard deviation of returns)\ntech_data &lt;- tech_data |&gt;\n  group_by(Source) |&gt;\n  mutate(volatility = slide_dbl(daily_returns, sd, .before = 19, .complete = TRUE)) |&gt;\n  ungroup()\n\n# Calculate average returns\navg_returns &lt;- tech_data |&gt;\n  group_by(Source) |&gt;\n  summarize(avg_daily_return = mean(daily_returns, na.rm = TRUE)) |&gt;\n  arrange(desc(avg_daily_return))\n\nprint(avg_returns)\n\n\n# A tibble: 5 × 2\n  Source                       avg_daily_return\n  &lt;chr&gt;                                   &lt;dbl&gt;\n1 NVDA historical data                    0.321\n2 MSFT Historical Data                    0.124\n3 META CLASS A Historical data            0.122\n4 AAPL Historical data                    0.119\n5 GOOG class C Historical data            0.100\n\n\nCode\ntech_data_clean &lt;- tech_data |&gt; filter(!is.na(daily_returns))\n\n# Plot daily returns\nggplot(tech_data_clean, aes(x = date, y = daily_returns, color = Source)) +\n  geom_line() +\n  labs(title = \"Daily Returns by Stock\", x = \"Date\", y = \"Daily Return (%)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nSimilar to the financial sector, there is extreme volatility due to covid-19. NVDA and META show noticeable spikes in returns, which indicates strong market reactions. After 2020, tech industry also experienced stability with occasional positive and negative spikes. Around 2022, META and MSFT experienced significant dips, attributing to major concerns in the world economy. It could either be due to internal struggles or external factors such as inflation, rising interest rates etc. In 2023, NVDA takes the spotlight again by having a noticeable increase in activity, due to the AI boom. Based on our analysis, we can see that more seasoned companies like AAPL and GOOG exhibit consistent behaviour with moderate spikes. META, MSFT and NVDA exhibit much more volatility which speaks for the risk associated with them.\n\n\nCode\n# Plot volatility\ntech_data_clean &lt;- tech_data |&gt; filter(!is.na(volatility))\n# ggplot(tech_data_clean, aes(x = date, y = volatility, color = Source)) +\n#   geom_line() +\n#   labs(title = \"Volatility (20-Day Rolling Standard Deviation) by Stock\", x = \"Date\", y = \"Volatility (%)\") +\n#   theme_minimal()\n\n\n\n\nCode\nlibrary(TTR)\n\ntech_data &lt;- tech_data |&gt;\n  group_by(Source) |&gt;\n  mutate(RSI = TTR::RSI(close, n = 14)) |&gt;\n  ungroup()\n\ntech_data &lt;- tech_data |&gt; filter(!is.na(RSI))\n\n# stock_names &lt;- unique(tech_data$Source)\n# for(stock in stock_names) {\n#   print(create_rsi_plot(tech_data, stock))\n# }\n\n\n\n\nCode\nlibrary(gridExtra)\nlibrary(grid)\nperformance_metrics &lt;- tech_data |&gt;\n  group_by(Source) |&gt;\n  summarize(\n    sharpe_ratio = mean(daily_returns, na.rm = TRUE) / \n                  sd(daily_returns, na.rm = TRUE) * sqrt(252),\n    max_drawdown = min(cumsum(daily_returns), na.rm = TRUE),\n    avg_volatility = mean(volatility, na.rm = TRUE)\n  ) |&gt;\n  arrange(desc(sharpe_ratio))\n\nprint(performance_metrics)\n\n\n# A tibble: 5 × 4\n  Source                       sharpe_ratio max_drawdown avg_volatility\n  &lt;chr&gt;                               &lt;dbl&gt;        &lt;dbl&gt;          &lt;dbl&gt;\n1 NVDA historical data                1.51         -2.03           3.18\n2 AAPL Historical data                0.940       -13.3            1.81\n3 GOOG class C Historical data        0.771       -21.1            1.89\n4 MSFT Historical Data                0.694       -46.7            2.55\n5 META CLASS A Historical data        0.684       -46.8            2.55\n\n\nCode\n# Calculate rolling metrics\ntech_data &lt;- tech_data |&gt;\n  group_by(Source) |&gt;\n  arrange(date) |&gt;\n  mutate(\n    # Rolling Sharpe Ratio (252-day window)\n    rolling_returns_mean = zoo::rollmean(daily_returns, k = 252, fill = NA),\n    rolling_returns_sd = zoo::rollapply(daily_returns, width = 252, FUN = sd, fill = NA),\n    rolling_sharpe = (rolling_returns_mean / rolling_returns_sd) * sqrt(252),\n    \n    # Rolling Maximum Drawdown\n    cumulative_return = cumprod(1 + daily_returns),\n    rolling_max = zoo::rollmax(cumulative_return, k = 252, fill = NA),\n    rolling_drawdown = (cumulative_return - rolling_max) / rolling_max\n  ) |&gt;\n  ungroup()\n\ntech_data &lt;- tech_data |&gt; filter(!is.na(rolling_sharpe))\ntech_data &lt;- tech_data |&gt; filter(!is.na(rolling_drawdown))\n\n\n# Create rolling Sharpe ratio plot\nsharpe_plot &lt;- ggplot(tech_data, aes(x = date, y = rolling_sharpe, color = Source)) +\n  geom_line() +\n  labs(title = \"Rolling Sharpe Ratio Over Time (252-day window)\",\n       x = \"Date\", y = \"Sharpe Ratio\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n# Arrange plots vertically\nggplot(tech_data, aes(x = date, y = rolling_sharpe, color = Source)) +\n  geom_line() +\n  labs(title = \"Rolling Sharpe Ratio Over Time\",\n       x = \"Date\", y = \"Sharpe Ratio\") +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\nThis ratio fluctuates significantly over time. For most of the stocks, there is a noticeable dip around 2022 signaling high market volatility or low returns. We can speculate that this could be because of the economic slowdown, political conflicts and global uncertainty due to a series of issues within the last 5 year period. NVDA has an outstanding Sharpe Ratio reflecting higher risk-adjusted performance despite having a negative ratio around 2022. Consequently, GOOG and AAPL had negative ratios but slower increases from the initial drastic dip, which indicated that the returns were not sufficient for the risk associated with them. Infact, AAPL’s performance increased in 2023 but dipped again near 2024.\nBut the gradual increase in the ratio from 2022 to 2023 indicates that the market is stabilizing and recovering.\n\n\nCode\ndrawdown_plot &lt;- ggplot(tech_data, aes(x = date, y = rolling_drawdown, color = Source)) +\n  geom_line() +\n  labs(title = \"Rolling Maximum Drawdown Over Time (252-day window)\",\n       x = \"Date\", y = \"Drawdown\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nggplot(tech_data, aes(x = date, y = rolling_drawdown)) +\n  geom_line() +\n  labs(title = \"Rolling Maximum Drawdown Over Time\",\n       x = \"Date\", y = \"Drawdown\") +\n  theme_minimal() +\n  theme(\n    panel.grid.minor = element_blank(),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title = element_text(face = \"bold\")\n  ) +\n  facet_wrap(~ Source)\n\n\n\n\n\n\n\n\n\nFor most stocks, the drawdowns dipped to the negatives suggesting a greater decline in value in relation to the previous peak.This clearly indicates market stress and concerns. Contrastingly, there are also periods of stability where the stocks have smaller drawdowns showcasing a stable performance. But, if we take a closer look at the major dips, they seem to have occurred at around the same time. This is where our events analysis comes in handy. If we analyse at what time these dips occurred, we can find a reason for this heavy market stress.\nImplications: There is a downside risk for each stock except NVDA, which maintained a flatline throughout the 5-year period. Stocks with deep dip or frequent drawdowns poses heavy risk during volatile periods. Comparing drawdown behaviour can help investors make proper decisions to optimize their portfolio.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "results.html#seasonal-patterns-and-cycles",
    "href": "results.html#seasonal-patterns-and-cycles",
    "title": "3  Results",
    "section": "4.6 4 Seasonal Patterns and Cycles",
    "text": "4.6 4 Seasonal Patterns and Cycles\n\n4.6.1 4.1. Healthcare\n\n\nCode\nlibrary(purrr)\n# Clean and prepare the data\nhealth_data &lt;- health_data |&gt;\n  filter(!is.na(date) & !is.na(close)) |&gt;  # Remove rows with missing Date or Price\n  mutate(Date = as.Date(date)) |&gt;          # Convert Date to Date format\n  arrange(Sector, Source, date)\n\n# Define a function to perform seasonal decomposition and Fourier analysis\nanalyze_seasonal_patterns &lt;- function(data, sector) {\n  # Filter data for the given sector\n  sector_data &lt;- data |&gt; filter(Sector == sector)\n\n  # Perform analysis for each stock\n  sector_analysis &lt;- sector_data |&gt;\n    group_by(Source) |&gt;\n    summarise(\n      decomposed_plot = list({\n        tryCatch({\n          # Convert to time series object\n          ts_data &lt;- ts(\n            close,\n            frequency = 365,\n            start = c(year(min(date)), yday(min(date)))\n          )\n\n          # Seasonal decomposition\n          decomposed &lt;- stl(ts_data, s.window = \"periodic\")\n\n          # Plot decomposition\n          autoplot(decomposed) + \n            ggtitle(paste(\"Seasonal Decomposition of\", sector, \"-\", unique(Source)))\n        }, error = function(e) {\n          message(paste(\"Error processing decomposition for stock:\", unique(Source)))\n          NULL\n        })\n      })\n    )\n\n  # Return analysis\n  return(sector_analysis)\n}\n\n# Perform analysis sector by sector\nsectors &lt;- unique(health_data$Sector)\nresults &lt;- map(sectors, ~ analyze_seasonal_patterns(health_data, .x))\n\n\nError processing decomposition for stock: AZN Historical Data\n\n\nError processing decomposition for stock: JNJ Historical data\n\n\nError processing decomposition for stock: LLY Historical Data\n\n\nError processing decomposition for stock: MRK Historical Data\n\n\nError processing decomposition for stock: PFE Historical Data\n\n\nCode\n# Save or display plots\nwalk2(sectors, results, function(sector, sector_result) {\n  message(paste(\"Processing sector:\", sector))\n  walk(sector_result$decomposed_plot, function(decomp_plot) {\n    if (!is.null(decomp_plot)) print(decomp_plot)\n  })\n})\n\n\nProcessing sector: Healthcare\n\n\nAZN and MRK have a steady upward trend but JNJ and PFE are prone to deviations and residual variability likely due to external factors.\nLLY demonstrates the strongest upward trend among all indicating its leadership in the healthcare industry.\nAll the stocks in healthcare show seasonality, because of frequent product launches and healthcare cycles.\n\n\n4.6.2 4.2 Finance\n\n\nCode\n# Clean and prepare the data\nbanking_data &lt;- banking_data |&gt;\n  filter(!is.na(date) & !is.na(close)) |&gt;  # Remove rows with missing Date or Price\n  mutate(Date = as.Date(date)) |&gt;          # Convert Date to Date format\n  arrange(Sector, Source, date)\n\n# Perform analysis sector by sector\nsectors &lt;- unique(banking_data$Sector)\nresults &lt;- map(sectors, ~ analyze_seasonal_patterns(banking_data, .x))\n\n\nError processing decomposition for stock: BAC Historical Data\n\n\nError processing decomposition for stock: GS Historical Data\n\n\nError processing decomposition for stock: JPM Historical Data\n\n\nError processing decomposition for stock: MS Historical Data\n\n\nError processing decomposition for stock: WFC Historical Data\n\n\nCode\n# Save or display plots\nwalk2(sectors, results, function(sector, sector_result) {\n  message(paste(\"Processing sector:\", sector))\n  walk(sector_result$decomposed_plot, function(decomp_plot) {\n    if (!is.null(decomp_plot)) print(decomp_plot)\n  })\n})\n\n\nProcessing sector: Banking\n\n\nAs we found in the previous graphs in the banking sector, there is a dip in 2022, which can be linked to poor economic conditions. In-spite of that WFC and MS exhibit steadier performance while GS and JPM are susceptible to market conditions. This could be because of the inherent nature of their business because their focus is on investment banking, trading, etc.\nSince, all the banks have quarterly earnings and fiscal cycles, the seasonal patterns are displayed by all the banks.\n\n\n4.6.3 4.3 Technology\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats 1.0.0     ✔ tibble  3.2.1\n✔ stringr 1.5.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ gridExtra::combine() masks dplyr::combine()\n✖ plotly::filter()     masks dplyr::filter(), stats::filter()\n✖ dplyr::lag()         masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(lubridate)\nlibrary(forecast)\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nCode\nlibrary(ggplot2)\nlibrary(tseries)\n\n# Clean and prepare the data\ntech_data &lt;- tech_data |&gt;\n  filter(!is.na(date) & !is.na(close)) |&gt;  # Remove rows with missing Date or Price\n  mutate(Date = as.Date(date)) |&gt;          # Convert Date to Date format\n  arrange(Sector, Source, date)\n\n\n# Perform analysis sector by sector\nsectors &lt;- unique(tech_data$Sector)\nresults &lt;- map(sectors, ~ analyze_seasonal_patterns(tech_data, .x))\n\n# Save or display plots\nwalk2(sectors, results, function(sector, sector_result) {\n  message(paste(\"Processing sector:\", sector))\n  walk(sector_result$decomposed_plot, function(decomp_plot) {\n    if (!is.null(decomp_plot)) print(decomp_plot)\n  })\n})\n\n\nProcessing sector: Technology\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExcept META, every other company exhibits a steady upward trend which reflects growth and investor confidence. We can see that MSFT and META prone to seasonal effects linked to industry cycles.\nNVDA has been growing very quickly and this can be seen from the graph as well, they show rapid growth and minimal seasonality due to their focus on innovative techniques.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "d3graph.html",
    "href": "d3graph.html",
    "title": "4  Interactive graph",
    "section": "",
    "text": "Sector: \nStock: \n\n\n\nPart 1: Stock Price Over Time with Annotations\n\n\n\n\n\n\nPart 2: Risk vs. Return Scatterplot\n\n\n\n\n\n\nPart 3: Recovery Analysis\n\n\n\n\n\nHover over an event marker to see details.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Interactive graph</span>"
    ]
  }
]